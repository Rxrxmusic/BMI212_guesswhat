{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b45f8b1-de5a-4924-bccf-d443a361d065",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yuyue01\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c26b8322-a04c-42a2-9680-d9eec2f97771",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdataset\u001b[0m/             model.ipynb                     val.csv\n",
      "\u001b[01;34mextracted_frames\u001b[0m/    test.csv                        val_dataset.pkl\n",
      "\u001b[01;34mextracted_frames_2\u001b[0m/  test_statistics.csv             yolov3.cfg\n",
      "\u001b[01;34mfeatures\u001b[0m/            test_statistics_multilayer.csv  yolov3.weights\n",
      "\u001b[01;34mfeatures_dinovitb\u001b[0m/   train.csv                       yy_test.ipynb\n",
      "\u001b[01;34mfeatures_dinovits\u001b[0m/   train_dataset.pkl\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9888763-bd86-49ea-b085-e839728c81fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98060620-ea96-41f2-9ab6-4d672defc1e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_REVIEWED/bangladesh_videos/%00_Good_GW_Gamplay/taniaislam56%40gmail.com/1563001715663/GuessWhat.mp4', '_REVIEWED/bangladesh_videos/%00_Good_GW_Gamplay/taniaislam56%40gmail.com/1563001844485/GuessWhat.mp4']\n"
     ]
    }
   ],
   "source": [
    "videos = []\n",
    "for file in [\"test.csv\", \"train.csv\", \"val.csv\"]:\n",
    "    with open(file, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        for row in csv_reader:\n",
    "            line = str(row).split(\" \")\n",
    "            path = \"/\".join(line[0].split(\"/\")[4:])\n",
    "            videos.append(path)\n",
    "print(videos[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0429c0a5-2f31-4f18-a198-c9d33d7f07fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f348436c-483b-47d9-b365-5e444629ac12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for video in videos:\n",
    "    path = \"dataset/guesswhat/\" + \"/\".join(video.split(\"/\")[:-1])\n",
    "    !mkdir -p {path}\n",
    "    !aws s3api get-object --bucket headsup-du1r3b78fy --key {video} dataset/guesswhat/{video} > download.txt\n",
    "    i += 1\n",
    "    if i % 10 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ea6ed330-8163-449a-894f-b00124a652ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Total frames in video: 1726\n",
      "-- Finished extracting frames from dataset/guesswhat/JustTxtFileORJustMp4/%2B12265686668/1585521787706/GuessWhat.mp4 to extracted_frames_32/JustTxtFileORJustMp4/%2B12265686668/1585521787706\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_frames(video_path, output_folder, num_frames=32):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"-- Error: Cannot open video file {video_path}\")\n",
    "        return\n",
    "\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if length == 0:\n",
    "        print(f\"-- Error: Video file {video_path} contains no frames.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"-- Total frames in video: {length}\")\n",
    "    frame_ids = [int(length / num_frames * i) for i in range(num_frames)]\n",
    "    # print(f\"Selected frame IDs: {frame_ids}\")\n",
    "\n",
    "    count = 0\n",
    "    frame_id = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count in frame_ids:\n",
    "            frame_filename = os.path.join(output_folder, f\"frame_{frame_id:02d}.jpg\")\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "            # print(f\"-- Saved frame {frame_id} to {frame_filename}\")\n",
    "            frame_id += 1\n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"-- Finished extracting frames from {video_path} to {output_folder}\")\n",
    "    print()\n",
    "\n",
    "# ex\n",
    "video_path = 'dataset/guesswhat/JustTxtFileORJustMp4/%2B12265686668/1585521787706/GuessWhat.mp4'\n",
    "output_folder = 'extracted_frames_32/JustTxtFileORJustMp4/%2B12265686668/1585521787706'\n",
    "extract_frames(video_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54144dbf-0d7e-482f-88fb-99a6e0e4f0df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Total frames in video: 1733\n",
      "-- Finished extracting and saving 16 frames from dataset/guesswhat/cumlupo%40gmail.com/1590047043296/GuessWhat.mp4 to extracted_frames_2/cumlupo%40gmail.com/1590047043296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add human detection when sampling\n",
    "\n",
    "def load_yolo_model(cfg_path, weights_path):\n",
    "    # Load YOLO model\n",
    "    net = cv2.dnn.readNet(weights_path, cfg_path)\n",
    "    layer_names = net.getLayerNames()\n",
    "    try:\n",
    "        output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    except IndexError:\n",
    "        output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    return net, output_layers\n",
    "\n",
    "def detect_humans(frame, net, output_layers):\n",
    "    height, width, channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if class_id == 0 and confidence > 0.5:  # class_id == 0 is for 'person'\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    return len(indexes) > 0\n",
    "\n",
    "def extract_frames_with_human_detection(video_path, output_folder, initial_num_frames=128, final_num_frames=16, cfg_path='yolov3.cfg', weights_path='yolov3.weights'):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    net, output_layers = load_yolo_model(cfg_path, weights_path)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"-- Error: Cannot open video file {video_path}\")\n",
    "        return\n",
    "\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if length == 0:\n",
    "        print(f\"-- Error: Video file {video_path} contains no frames.\")\n",
    "        return\n",
    "\n",
    "    print(f\"-- Total frames in video: {length}\")\n",
    "    frame_ids = [int(length / initial_num_frames * i) for i in range(initial_num_frames)]\n",
    "\n",
    "    count = 0\n",
    "    frame_id = 0\n",
    "    detected_frames = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count in frame_ids:\n",
    "            if detect_humans(frame, net, output_layers):\n",
    "                detected_frames.append((frame_id, frame))\n",
    "                # print(f\"-- Detected human in frame {frame_id}\")\n",
    "            frame_id += 1\n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Select 16 frames evenly spaced among all detected frames\n",
    "    total_detected = len(detected_frames)\n",
    "    if total_detected < final_num_frames:\n",
    "        print(f\"-- Warning: Only {total_detected} frames with human detections found, less than {final_num_frames} required.\")\n",
    "        selected_frames = detected_frames\n",
    "    else:\n",
    "        interval = total_detected // final_num_frames\n",
    "        selected_frames = [detected_frames[i * interval] for i in range(final_num_frames)]\n",
    "    \n",
    "    for i, (frame_id, frame) in enumerate(selected_frames):\n",
    "        frame_filename = os.path.join(output_folder, f\"frame_{i:02d}.jpg\")\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "        # print(f\"-- Saved frame {i} with human detection to {frame_filename}\")\n",
    "\n",
    "    print(f\"-- Finished extracting and saving {len(selected_frames)} frames from {video_path} to {output_folder}\")\n",
    "    print()\n",
    "\n",
    "# video_path = 'dataset/guesswhat/_REVIEWED/Testing_ProcessComplete/shimmer_96%40yahoo.com/1569790868469/GuessWhat.mp4'\n",
    "# output_folder = 'extracted_frames_32/_REVIEWED/Testing_ProcessComplete/shimmer_96%40yahoo.com/1569790868469'\n",
    "# extract_frames_with_human_detection(video_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c1232d03-f087-4845-8db8-bc55b62ffed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_videos(csv_file, base_video_path, output_base_path):\n",
    "    with open(csv_file, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "\n",
    "        for row in csv_reader:\n",
    "            line = str(row).split(\" \")\n",
    "            # video_path, label = line\n",
    "            video_path = line[0].strip().strip(\"['\")\n",
    "            relative_path = \"/\".join(video_path.split(\"/\")[4:])\n",
    "            full_video_path = os.path.join(base_video_path, relative_path)\n",
    "            output_folder = os.path.join(output_base_path, \"/\".join(relative_path.split(\"/\")[:-1]))\n",
    "            \n",
    "            if not os.path.isdir(output_folder):\n",
    "                os.makedirs(output_folder)\n",
    "            else:\n",
    "                print(\"Video file already exists\")\n",
    "                continue\n",
    "                    \n",
    "                \n",
    "            print(\"Processing video:\", full_video_path)\n",
    "            print(\"Output folder:\", output_folder)\n",
    "            \n",
    "            if os.path.isfile(full_video_path):\n",
    "                extract_frames(full_video_path, output_folder)\n",
    "                # extract_frames_with_human_detection(full_video_path, output_folder)\n",
    "            else:\n",
    "                print(\"Video file does not exist:\", full_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c1366-a811-4fdc-8af0-21c90b2b49b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extracting all frames from all videos: takes a while, only extract frames once\n",
    "base_video_path = 'dataset/guesswhat'\n",
    "output_base_path = 'extracted_frames_32'\n",
    "for file in [\"train.csv\", \"test.csv\", \"val.csv\"]:\n",
    "    process_videos(file, base_video_path, output_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a0011ab3-bb45-4b4b-966f-143ced2be670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "29e37923-5631-4ef2-a43c-f7dde3e1b6ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the pre-processing transformation and load foundation model\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def load_pretrained_dinov2_model():\n",
    "    model = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
    "    # model = torch.hub.load('facebookresearch/dino:main', 'dino_vitb16')\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "324d0557-02dc-4ed3-85c7-5bf45169d263",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/yuyue01/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "foundation_model = load_pretrained_dinov2_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "001c80b8-268c-478d-a2a9-bad601f01b5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_and_concatenate_features(model, frames_folder):\n",
    "    frame_files = [f for f in sorted(os.listdir(frames_folder)) if f.endswith(\".jpg\")]\n",
    "\n",
    "    if len(frame_files) != 32: # 16\n",
    "        print(f\"-- Error: Expected 32 (or 16) frames, but found {len(frame_files)} frames in folder: {frames_folder}\")\n",
    "        return None\n",
    "    \n",
    "    feature_list = []\n",
    "    for frame_file in frame_files:\n",
    "        if frame_file.endswith(\".jpg\"): \n",
    "            frame_path = os.path.join(frames_folder, frame_file)\n",
    "            image = Image.open(frame_path).convert('RGB')\n",
    "            image_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "            with torch.no_grad():\n",
    "                features = model(image_tensor)\n",
    "            feature_list.append(features.squeeze(0).numpy())  # Remove batch dimension\n",
    "            \n",
    "    if not feature_list:  # Check if feature_list is empty\n",
    "        print(f\"No features extracted in folder: {frames_folder}\")\n",
    "        return None\n",
    "    \n",
    "    concatenated_features = np.concatenate(feature_list, axis=0)\n",
    "    print(\"Finished extracting and concatenating features in folder:\", frames_folder)\n",
    "    return concatenated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e6e3b2d6-2203-4ae7-a752-02b7849a0c19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_features(features, output_path):\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    np.save(output_path, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "0d6b3b51-9f98-4c63-acea-ef2d3424438c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_all_frames(csv_file, base_extracted_frames_path, features_output_base_path, model):\n",
    "    with open(csv_file, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        \n",
    "        for row in csv_reader:\n",
    "            line = str(row).split(\" \")\n",
    "            # video_path, label = line\n",
    "            video_path = line[0].strip().strip(\"['\")\n",
    "            relative_path = \"/\".join(video_path.split(\"/\")[4:])\n",
    "            frames_folder = os.path.join(base_extracted_frames_path, \"/\".join(relative_path.split(\"/\")[:-1]))\n",
    "            output_path = os.path.join(features_output_base_path, \"/\".join(relative_path.split(\"/\")[:-1]), \"features.npy\")\n",
    "            \n",
    "#             if os.path.isfile(output_path):\n",
    "#                 print(f\"Output path already exists.\")\n",
    "#                 continue\n",
    "            \n",
    "            if os.path.isdir(frames_folder):\n",
    "                print(\"Processing frames in folder:\", frames_folder)\n",
    "                \n",
    "                # extract features from frames using foundation model\n",
    "                concatenated_features = extract_and_concatenate_features(model, frames_folder)\n",
    "                if concatenated_features is not None:\n",
    "                    save_features(concatenated_features, output_path)\n",
    "                    print(f\"Saved aggregated features to {output_path}\")\n",
    "                else:\n",
    "                    print(f\"Skipping saving for {frames_folder} due to no extracted features.\")\n",
    "                print()\n",
    "            else:\n",
    "                print(\"Frames folder does not exist:\", frames_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4034de22-5f98-4c24-85af-0b6d0cef3e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from frames for all videos (!!!takes a while)\n",
    "base_extracted_frames_path = 'extracted_frames_32'\n",
    "features_output_base_path = 'features_dinovits_32'\n",
    "for file in [\"train.csv\", \"test.csv\", \"val.csv\"]:\n",
    "    process_all_frames(file, base_extracted_frames_path, features_output_base_path, foundation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "fff64ea6-0274-47b9-acb7-3b95994a6f93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2ddd1b92-d3ca-4495-b72a-ccf46a9cdcd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "class VideoFeatureDataset(Dataset):\n",
    "    def __init__(self, features_base_path, csv_file=None, preloaded_data=None):\n",
    "        self.features_base_path = features_base_path\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        if preloaded_data:\n",
    "            self.data, self.labels = preloaded_data\n",
    "        elif csv_file:\n",
    "            with open(csv_file, 'r') as f:\n",
    "                csv_reader = csv.reader(f)\n",
    "                for row in csv_reader:\n",
    "                    line = str(row).split(\" \")\n",
    "                    video_path = line[0].strip().strip(\"['\")\n",
    "                    label = line[1].strip().strip(\"']\")\n",
    "                    relative_path = \"/\".join(video_path.split(\"/\")[4:])\n",
    "                    feature_file = os.path.join(features_base_path, \"/\".join(relative_path.split(\"/\")[:-1]), \"features.npy\")\n",
    "                    if os.path.isfile(feature_file):\n",
    "                        self.data.append(feature_file)\n",
    "                        self.labels.append(int(label))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature_file = self.data[idx]\n",
    "        # load feature numpy\n",
    "        features = np.load(feature_file)\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(features, dtype=torch.float32), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ad6986fa-2172-4e4c-9975-4d8dc65a66e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_dataset(dataset, file_path):\n",
    "    data = {'data': dataset.data, 'labels': dataset.labels}\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data['data'], data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d63351a1-add6-4cc9-8fae-bed8b2d034eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "train_csv = 'train.csv'\n",
    "val_csv = 'val.csv'\n",
    "features_base_path = 'features_dinovits_32'\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "004aed26-fb2a-4078-80d5-b899a2d77dc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First time - Create datasets and dataloaders\n",
    "train_dataset = VideoFeatureDataset(features_base_path, train_csv)\n",
    "val_dataset = VideoFeatureDataset(features_base_path, val_csv)\n",
    "# Save datasets\n",
    "save_dataset(train_dataset, 'train_dataset.pkl')\n",
    "save_dataset(val_dataset, 'val_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6075ac1f-b9da-487a-9ee8-cd0681433841",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset loaded successfully: 228\n",
      "Validation dataset loaded successfully: 47\n",
      "First few training items: ['features_dinovits_32/billypano@hotmail.com/1622504001/features.npy', 'features_dinovits_32/billypano@hotmail.com/1622504303/features.npy', 'features_dinovits_32/billypano@hotmail.com/1622504663/features.npy'] [0, 0, 0]\n",
      "First few validation items: ['features_dinovits_32/_REVIEWED/Testing_ProcessComplete/fardhanaalam%40gmail.com/1560171503917/features.npy', 'features_dinovits_32/_REVIEWED/bangladesh_videos/%02_Flawed_Gameplay/%2B8801925832996/1563950970528/features.npy', 'features_dinovits_32/%2B16383224499/1607082139208/features.npy'] [0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_data, train_labels = load_dataset('train_dataset.pkl')\n",
    "    val_data, val_labels = load_dataset('val_dataset.pkl')\n",
    "    print(\"Train dataset loaded successfully:\", len(train_data))\n",
    "    print(\"Validation dataset loaded successfully:\", len(val_data))\n",
    "\n",
    "    # check the first few items\n",
    "    print(\"First few training items:\", train_data[:3], train_labels[:3])\n",
    "    print(\"First few validation items:\", val_data[:3], val_labels[:3])\n",
    "except Exception as e:\n",
    "    print(\"Error loading datasets:\", e)\n",
    "\n",
    "# Create datasets using preloaded data\n",
    "train_dataset = VideoFeatureDataset(features_base_path, preloaded_data=(train_data, train_labels))\n",
    "val_dataset = VideoFeatureDataset(features_base_path, preloaded_data=(val_data, val_labels))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "ed18528e-4240-40f5-894f-2c74f9ef2b1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SingleLayerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SingleLayerClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Parameters\n",
    "# input_dim = 6144\n",
    "input_dim = 12288\n",
    "num_classes = 2 \n",
    "\n",
    "# Instantiate the classifier\n",
    "clf = SingleLayerClassifier(input_dim, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "a5d12e13-9b1d-4768-bb4b-22e640bd40b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EnhancedClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(EnhancedClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.fc4 = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Parameters\n",
    "# input_dim = 6144 \n",
    "input_dim = 12288\n",
    "num_classes = 2 \n",
    "# Instantiate the model\n",
    "clf = EnhancedClassifier(input_dim, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "d2e729dc-9c30-46dc-982e-1b84f8d886e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "batch_size = 16\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(clf.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "339fc848-3f78-445a-9e54-11bd4e777b44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "819c8a3a-1639-4dd1-b3f1-1cf6e75575a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.7597\n",
      "Validation Loss: 0.6953, Accuracy: 53.19%, Precision: 0.5669, Recall: 0.5319, F1 Score: 0.5336\n",
      "Epoch 2/20, Loss: 0.6930\n",
      "Validation Loss: 0.6767, Accuracy: 55.32%, Precision: 0.5570, Recall: 0.5532, F1 Score: 0.5549\n",
      "Epoch 3/20, Loss: 0.6163\n",
      "Validation Loss: 0.6549, Accuracy: 57.45%, Precision: 0.5545, Recall: 0.5745, F1 Score: 0.5552\n",
      "Epoch 4/20, Loss: 0.6313\n",
      "Validation Loss: 0.6606, Accuracy: 57.45%, Precision: 0.5478, Recall: 0.5745, F1 Score: 0.5443\n",
      "Epoch 5/20, Loss: 0.5501\n",
      "Validation Loss: 0.6716, Accuracy: 53.19%, Precision: 0.4581, Recall: 0.5319, F1 Score: 0.4652\n",
      "Epoch 6/20, Loss: 0.4967\n",
      "Validation Loss: 0.6841, Accuracy: 53.19%, Precision: 0.4581, Recall: 0.5319, F1 Score: 0.4652\n",
      "Epoch 7/20, Loss: 0.5181\n",
      "Validation Loss: 0.6829, Accuracy: 57.45%, Precision: 0.5163, Recall: 0.5745, F1 Score: 0.4929\n",
      "Epoch 8/20, Loss: 0.4808\n",
      "Validation Loss: 0.6768, Accuracy: 46.81%, Precision: 0.4029, Recall: 0.4681, F1 Score: 0.4224\n",
      "Epoch 9/20, Loss: 0.5072\n",
      "Validation Loss: 0.6750, Accuracy: 51.06%, Precision: 0.4371, Recall: 0.5106, F1 Score: 0.4511\n",
      "Epoch 10/20, Loss: 0.4824\n",
      "Validation Loss: 0.6740, Accuracy: 51.06%, Precision: 0.4371, Recall: 0.5106, F1 Score: 0.4511\n",
      "Epoch 11/20, Loss: 0.4793\n",
      "Validation Loss: 0.6744, Accuracy: 55.32%, Precision: 0.4835, Recall: 0.5532, F1 Score: 0.4791\n",
      "Epoch 12/20, Loss: 0.4972\n",
      "Validation Loss: 0.6713, Accuracy: 53.19%, Precision: 0.4581, Recall: 0.5319, F1 Score: 0.4652\n",
      "Epoch 13/20, Loss: 0.4534\n",
      "Validation Loss: 0.6792, Accuracy: 57.45%, Precision: 0.5163, Recall: 0.5745, F1 Score: 0.4929\n",
      "Epoch 14/20, Loss: 0.4887\n",
      "Validation Loss: 0.6825, Accuracy: 53.19%, Precision: 0.4581, Recall: 0.5319, F1 Score: 0.4652\n",
      "Epoch 15/20, Loss: 0.4693\n",
      "Validation Loss: 0.6807, Accuracy: 55.32%, Precision: 0.4835, Recall: 0.5532, F1 Score: 0.4791\n",
      "Epoch 16/20, Loss: 0.4485\n",
      "Validation Loss: 0.6830, Accuracy: 59.57%, Precision: 0.5623, Recall: 0.5957, F1 Score: 0.5066\n",
      "Epoch 17/20, Loss: 0.4932\n",
      "Validation Loss: 0.6817, Accuracy: 55.32%, Precision: 0.4835, Recall: 0.5532, F1 Score: 0.4791\n",
      "Epoch 18/20, Loss: 0.5095\n",
      "Validation Loss: 0.6766, Accuracy: 55.32%, Precision: 0.4835, Recall: 0.5532, F1 Score: 0.4791\n",
      "Epoch 19/20, Loss: 0.4842\n",
      "Validation Loss: 0.6808, Accuracy: 53.19%, Precision: 0.4797, Recall: 0.5319, F1 Score: 0.4838\n",
      "Epoch 20/20, Loss: 0.4855\n",
      "Validation Loss: 0.6760, Accuracy: 55.32%, Precision: 0.4835, Recall: 0.5532, F1 Score: 0.4791\n"
     ]
    }
   ],
   "source": [
    "# Training....\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    clf.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = clf(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    clf.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = clf(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    accuracy = 100 * correct / total\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}\")\n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "2dc3cd1d-b82e-4616-97db-001b4cfb5c57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6225, Accuracy: 68.18%, Precision: 0.7092, Recall: 0.6818, F1 Score: 0.6515\n",
      "Test statistics saved to test_statistics_multilayer.csv\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the clf\n",
    "test_csv = 'test.csv'\n",
    "features_base_path = 'features_dinovits_32'\n",
    "test_dataset = VideoFeatureDataset(features_base_path, test_csv)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "clf.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "test_statistics = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = clf(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        # Collect statistics\n",
    "        for i in range(len(labels)):\n",
    "            test_statistics.append([test_dataset.data[i], labels[i].item(), predicted[i].item()])\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = 100 * correct / total\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n",
    "print(f\"Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}\")\n",
    "\n",
    "# Save test statistics to a CSV file\n",
    "output_csv = 'test_statistics_multilayer.csv'\n",
    "with open(output_csv, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Path\", \"True Label\", \"Predicted Label\"])\n",
    "    writer.writerows(test_statistics)\n",
    "\n",
    "print(f\"Test statistics saved to {output_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
